{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RemoteRL Quick-Start Tutorial\n",
        "\n",
        "RemoteRL lets you connect environment simulators and RL trainers over the internet via secure WebSockets Cloud Server.\n",
        "With a one-line `remoterl.init()` call, you can keep environments and trainers on different machines.\n",
        "\n",
        "**In this notebook you will:**\n",
        "- Install RemoteRL and dependencies\n",
        "- Launch a CartPole simulator process\n",
        "- Train a PPO agent remotely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['init', 'shutdown']\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet remoterl stable-baselines3\n",
        "import remoterl, stable_baselines3 as sb3\n",
        "print(remoterl.__all__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febc415f",
      "metadata": {},
      "source": [
        "## Get an API Key\n",
        "\n",
        "Create a free RemoteRL account and obtain your key by running the CLI locally:\n",
        "```bash\n",
        "remoterl register\n",
        "```\n",
        "Every account includes **1 GB of free credit**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6f62c3b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single-call helper that can (optionally) open the RemoteRL dashboard, grab or reuse your API key, \n",
        "# stash it in REMOTERL_API_KEY for the current session, and print a success message.\n",
        "\n",
        "import os, webbrowser\n",
        "DASHBOARD_URL = \"https://remoterl.com/user/dashboard\"\n",
        "\n",
        "def register_api_key(open_browser=True):\n",
        "    if open_browser:\n",
        "        webbrowser.open_new_tab(DASHBOARD_URL)\n",
        "        key = input(\"Paste your REMOTERL API key: \").strip()\n",
        "        os.environ[\"REMOTERL_API_KEY\"] = key\n",
        "    else:\n",
        "        key = os.getenv(\"REMOTERL_API_KEY\")\n",
        "    if not key:\n",
        "        print(f\"Please visit {DASHBOARD_URL} to get your API key.\")\n",
        "        raise RuntimeError(\"API key required.\")\n",
        "\n",
        "    print(\"‚úÖ RemoteRL registered. Happy training!\")\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38d6b301",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RemoteRL registered. Happy training!\n"
          ]
        }
      ],
      "source": [
        "# If the browser window doesn‚Äôt open automatically, visit the dashboard:\n",
        "DASHBOARD_URL = \"https://remoterl.com/user/dashboard\"\n",
        "\n",
        "key = register_api_key(open_browser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch a Remote Simulator\n",
        "\n",
        "Launches a minimal RemoteRL simulator cluster and waits for trainers to connect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f21f3d89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ  Simulator subprocess started (pid=40216)\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Simulator started, waiting for connection to Trainers...\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sim] \u001b[38;5;71m[RemoteRL] Connected | trainer=99db70db\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Session started | trainer=99db70db | num_env_runners=2\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=1 make:{'0': 'CartPole-v1'} | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=1 make:{'0': 'CartPole-v1'} | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=8 make:{'7': 'CartPole-v1'} | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=8 make:{'7': 'CartPole-v1'} | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;117m[RemoteRL] | simulator |    520 MB left | https://remoterl.com/user/dashboard | elapsed 0:00:00 | ~ calculating...\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=64 step | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=64 step | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=512 step | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=512 step | runner=1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ Cell A ¬∑ starts an isolated simulator (works on Windows, Linux, macOS) ‚îÄ‚îÄ\n",
        "import os, sys, textwrap, subprocess, time\n",
        "import threading\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Grab API key from the environment (set earlier by register_api_key)\n",
        "# ------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"REMOTERL_API_KEY\", key)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Build one‚Äëliner Python code that will run inside the child process\n",
        "#     (`remoterl.init(..., role=\"simulator\")` is intentionally blocking)\n",
        "# ------------------------------------------------------------------\n",
        "sim_code = textwrap.dedent(f\"\"\"\n",
        "    import remoterl\n",
        "    \n",
        "    remoterl.init(api_key='{API_KEY}', role='simulator')   # blocks here(Simulator init designed to be blocking)\n",
        "\"\"\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Spawn the simulator subprocess (same Python executable, unbuffered)\n",
        "#     - stdout/stderr are piped so we can echo logs back in real time\n",
        "# ------------------------------------------------------------------\n",
        "sim_proc = subprocess.Popen(\n",
        "    [sys.executable, \"-u\", \"-c\", sim_code],\n",
        "    stdout=subprocess.PIPE,            # stream simulator logs to parent\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Background thread: continuously forward simulator output\n",
        "# ------------------------------------------------------------------\n",
        "def stream_logs(proc):\n",
        "    for line in iter(proc.stdout.readline, ''):   # keep reading until EOF\n",
        "        if line:\n",
        "            print(f\"[sim] {line.rstrip()}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£  Confirmation + optional head‚Äëstart delay\n",
        "# ------------------------------------------------------------------\n",
        "log_thread = threading.Thread(target=stream_logs, args=(sim_proc,), daemon=True)\n",
        "log_thread.start()\n",
        "\n",
        "print(f\"üöÄ  Simulator subprocess started (pid={sim_proc.pid})\")\n",
        "\n",
        "time.sleep(10)                          # give it a head-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start the Online Trainer\n",
        "\n",
        "Connects to the RemoteRL simulator cluster, instantiates a PPO agent, and trains online."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;71m[RemoteRL] Session opened | trainer=99db70db | num_env_runners=2 | num_workers=1\u001b[0m\n",
            "\u001b[94m[RemoteRL] Remote Gym enabled with 1 workers and 2 runners.\u001b[0m\n",
            "\u001b[94m[RemoteRL] Remote Stable-Baselines3 applied.\u001b[0m\n",
            "Using cpu device\n",
            "\u001b[38;5;117m[RemoteRL] | trainer |    524 MB left | https://remoterl.com/user/dashboard | elapsed 0:00:00 | ~ calculating...\u001b[0m\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 24.6     |\n",
            "|    ep_rew_mean     | 24.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 147      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 13       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 24.5         |\n",
            "|    ep_rew_mean          | 24.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 146          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020874795 |\n",
            "|    clip_fraction        | 0.00061      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.692       |\n",
            "|    explained_variance   | -0.00188     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.6         |\n",
            "|    n_updates            | 4            |\n",
            "|    policy_gradient_loss | -0.00123     |\n",
            "|    value_loss           | 75.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 28.2        |\n",
            "|    ep_rew_mean          | 28.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 148         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 41          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004819851 |\n",
            "|    clip_fraction        | 0.00977     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.684      |\n",
            "|    explained_variance   | -0.023      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.57        |\n",
            "|    n_updates            | 8           |\n",
            "|    policy_gradient_loss | -0.00438    |\n",
            "|    value_loss           | 40.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 31.1         |\n",
            "|    ep_rew_mean          | 31.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 152          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 53           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026214374 |\n",
            "|    clip_fraction        | 0.00549      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.673       |\n",
            "|    explained_variance   | -0.0321      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19.5         |\n",
            "|    n_updates            | 12           |\n",
            "|    policy_gradient_loss | -0.00322     |\n",
            "|    value_loss           | 41.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 41.5        |\n",
            "|    ep_rew_mean          | 41.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 155         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004175611 |\n",
            "|    clip_fraction        | 0.0115      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.664      |\n",
            "|    explained_variance   | 0.0953      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21.6        |\n",
            "|    n_updates            | 16          |\n",
            "|    policy_gradient_loss | -0.00325    |\n",
            "|    value_loss           | 59.4        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 47.7         |\n",
            "|    ep_rew_mean          | 47.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 159          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037332526 |\n",
            "|    clip_fraction        | 0.0138       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.645       |\n",
            "|    explained_variance   | 0.0507       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 33.4         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00269     |\n",
            "|    value_loss           | 67.8         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 55.9        |\n",
            "|    ep_rew_mean          | 55.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 88          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003803897 |\n",
            "|    clip_fraction        | 0.0122      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.644      |\n",
            "|    explained_variance   | 0.00348     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 42.6        |\n",
            "|    n_updates            | 24          |\n",
            "|    policy_gradient_loss | -0.00243    |\n",
            "|    value_loss           | 81.8        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 57.5         |\n",
            "|    ep_rew_mean          | 57.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 167          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 97           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033582486 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.622       |\n",
            "|    explained_variance   | 0.0542       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 27.8         |\n",
            "|    n_updates            | 28           |\n",
            "|    policy_gradient_loss | -0.008       |\n",
            "|    value_loss           | 81.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 66.8         |\n",
            "|    ep_rew_mean          | 66.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 171          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 107          |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030881732 |\n",
            "|    clip_fraction        | 0.00757      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.627       |\n",
            "|    explained_variance   | 0.197        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 27.2         |\n",
            "|    n_updates            | 32           |\n",
            "|    policy_gradient_loss | -0.00514     |\n",
            "|    value_loss           | 77.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 89.7         |\n",
            "|    ep_rew_mean          | 89.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 172          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 118          |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028895093 |\n",
            "|    clip_fraction        | 0.0117       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.617       |\n",
            "|    explained_variance   | 0.297        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 26.1         |\n",
            "|    n_updates            | 36           |\n",
            "|    policy_gradient_loss | -0.00108     |\n",
            "|    value_loss           | 74.2         |\n",
            "------------------------------------------\n",
            "‚úÖ Training finished.\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ Cell B ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import remoterl, os\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "# ------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Retrieve API key injected by the helper or set in the shell\n",
        "# ------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"REMOTERL_API_KEY\", key)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Connect to the RemoteRL backend in trainer mode\n",
        "# ------------------------------------------------------------------\n",
        "if not remoterl.init(api_key=API_KEY, role=\"trainer\"):\n",
        "    raise RuntimeError(\"Failed to connect to RemoteRL.\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Build a vectorised CartPole environment (32 parallel instances)\n",
        "# ------------------------------------------------------------------\n",
        "ENV_ID = \"CartPole-v1\"\n",
        "env     = make_vec_env(ENV_ID, n_envs=32)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ Instantiate PPO with a modest network architecture\n",
        "# ------------------------------------------------------------------\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    policy_kwargs=dict(net_arch=dict(pi=[128, 64], vf=[128, 64])),\n",
        "    n_steps=64, n_epochs=4, batch_size=64, verbose=1, device=\"auto\",\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ Train for roughly 20k environment steps\n",
        "# ------------------------------------------------------------------\n",
        "model.learn(total_timesteps=20_000)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ Graceful shutdown of envs and confirmation message\n",
        "# ------------------------------------------------------------------\n",
        "env.close()\n",
        "print(\"‚úÖ Training finished.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tester",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
