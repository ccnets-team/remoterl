{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RemoteRL Quick-Start Tutorial\n",
        "\n",
        "RemoteRL lets you connect environment simulators and RL trainers over the internet via secure WebSockets Cloud Server.\n",
        "With a one-line `remoterl.init()` call, you can keep environments and trainers on different machines.\n",
        "\n",
        "**In this notebook you will:**\n",
        "- Install RemoteRL and dependencies\n",
        "- Launch a CartPole simulator process\n",
        "- Train a PPO agent remotely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['init', 'shutdown']\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet remoterl stable-baselines3\n",
        "import remoterl, stable_baselines3 as sb3\n",
        "print(remoterl.__all__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febc415f",
      "metadata": {},
      "source": [
        "## Get an API Key\n",
        "\n",
        "Create a free RemoteRL account and obtain your key by running the CLI locally:\n",
        "```bash\n",
        "remoterl register\n",
        "```\n",
        "Every account includes **1 GB of free credit**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f62c3b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single-call helper that can (optionally) open the RemoteRL dashboard, grab or reuse your API key, \n",
        "# stash it in REMOTERL_API_KEY for the current session, and print a success message.\n",
        "\n",
        "import os, webbrowser\n",
        "DASHBOARD_URL = \"https://remoterl.com/user/dashboard\"\n",
        "\n",
        "def register_api_key(open_browser=True):\n",
        "    if open_browser:\n",
        "        webbrowser.open_new_tab(DASHBOARD_URL)\n",
        "        key = input(\"Paste your REMOTERL API key: \").strip()\n",
        "        os.environ[\"REMOTERL_API_KEY\"] = key\n",
        "    else:\n",
        "        key = os.getenv(\"REMOTERL_API_KEY\")\n",
        "    if not key:\n",
        "        print(f\"Please visit {DASHBOARD_URL} to get your API key.\")\n",
        "        raise RuntimeError(\"API key required.\")\n",
        "\n",
        "    print(\"‚úÖ RemoteRL registered. Happy training!\")\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38d6b301",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RemoteRL registered. Happy training!\n"
          ]
        }
      ],
      "source": [
        "key = register_api_key(open_browser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch a Remote Simulator\n",
        "\n",
        "Launches a minimal RemoteRL simulator cluster and waits for trainers to connect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21f3d89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ  Simulator subprocess started (pid=34688)\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Simulator started, waiting for connection to Trainers...\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sim] \u001b[38;5;71m[RemoteRL] Connected | trainer=da3f8d5b\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Session started | trainer=da3f8d5b | num_env_runners=2\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=1 make:{'0': 'CartPole-v1'} | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=1 make:{'0': 'CartPole-v1'} | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=8 make:{'7': 'CartPole-v1'} | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=8 make:{'7': 'CartPole-v1'} | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=64 step | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=64 step | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=512 step | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=512 step | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;117m[RemoteRL] | simulator |   5371 MB left | https://remoterl.com/user/dashboard | elapsed 0:00:00 | ~ calculating...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ Cell A ¬∑ starts an isolated simulator (works on Windows, Linux, macOS) ‚îÄ‚îÄ\n",
        "import os, sys, textwrap, subprocess, time\n",
        "import threading\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Grab API key from the environment (set earlier by register_api_key)\n",
        "# ------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"REMOTERL_API_KEY\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Build one‚Äëliner Python code that will run inside the child process\n",
        "#     (`remoterl.init(..., role=\"simulator\")` is intentionally blocking)\n",
        "# ------------------------------------------------------------------\n",
        "sim_code = textwrap.dedent(f\"\"\"\n",
        "    import remoterl\n",
        "    \n",
        "    remoterl.init(api_key='{API_KEY}', role='simulator')   # blocks here(Simulator init designed to be blocking)\n",
        "\"\"\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Spawn the simulator subprocess (same Python executable, unbuffered)\n",
        "#     - stdout/stderr are piped so we can echo logs back in real time\n",
        "# ------------------------------------------------------------------\n",
        "sim_proc = subprocess.Popen(\n",
        "    [sys.executable, \"-u\", \"-c\", sim_code],\n",
        "    stdout=subprocess.PIPE,            # stream simulator logs to parent\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Background thread: continuously forward simulator output\n",
        "# ------------------------------------------------------------------\n",
        "def stream_logs(proc):\n",
        "    for line in iter(proc.stdout.readline, ''):   # keep reading until EOF\n",
        "        if line:\n",
        "            print(f\"[sim] {line.rstrip()}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£  Confirmation + optional head‚Äëstart delay\n",
        "# ------------------------------------------------------------------\n",
        "log_thread = threading.Thread(target=stream_logs, args=(sim_proc,), daemon=True)\n",
        "log_thread.start()\n",
        "\n",
        "print(f\"üöÄ  Simulator subprocess started (pid={sim_proc.pid})\")\n",
        "\n",
        "time.sleep(10)                          # give it a head-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start the Online Trainer\n",
        "\n",
        "Connects to the RemoteRL simulator cluster, instantiates a PPO agent, and trains online."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;71m[RemoteRL] Session opened | trainer=da3f8d5b | num_env_runners=2 | num_workers=1\u001b[0m\n",
            "\u001b[94m[RemoteRL] Remote Gym enabled with 1 workers and 2 runners.\u001b[0m\n",
            "\u001b[94m[RemoteRL] Remote Stable-Baselines3 applied.\u001b[0m\n",
            "Using cpu device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 21.4     |\n",
            "|    ep_rew_mean     | 21.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 139      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 23.6         |\n",
            "|    ep_rew_mean          | 23.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 140          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051870225 |\n",
            "|    clip_fraction        | 0.0209       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.691       |\n",
            "|    explained_variance   | -0.0119      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 27.2         |\n",
            "|    n_updates            | 4            |\n",
            "|    policy_gradient_loss | -0.00428     |\n",
            "|    value_loss           | 75.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 30.3         |\n",
            "|    ep_rew_mean          | 30.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 146          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 41           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041428404 |\n",
            "|    clip_fraction        | 0.0167       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.681       |\n",
            "|    explained_variance   | -0.0336      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.5         |\n",
            "|    n_updates            | 8            |\n",
            "|    policy_gradient_loss | -0.0063      |\n",
            "|    value_loss           | 38.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 33.6         |\n",
            "|    ep_rew_mean          | 33.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 147          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 55           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045038615 |\n",
            "|    clip_fraction        | 0.00818      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.678       |\n",
            "|    explained_variance   | 0.0336       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 27.3         |\n",
            "|    n_updates            | 12           |\n",
            "|    policy_gradient_loss | -0.00312     |\n",
            "|    value_loss           | 50           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 33.9         |\n",
            "|    ep_rew_mean          | 33.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 149          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 68           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044795997 |\n",
            "|    clip_fraction        | 0.0178       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.655       |\n",
            "|    explained_variance   | 0.0242       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 30.6         |\n",
            "|    n_updates            | 16           |\n",
            "|    policy_gradient_loss | -0.0034      |\n",
            "|    value_loss           | 54           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 38.7         |\n",
            "|    ep_rew_mean          | 38.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 153          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 80           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027359088 |\n",
            "|    clip_fraction        | 0.0103       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.646       |\n",
            "|    explained_variance   | 0.128        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 30           |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.0029      |\n",
            "|    value_loss           | 64.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 44           |\n",
            "|    ep_rew_mean          | 44           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 157          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 90           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038052758 |\n",
            "|    clip_fraction        | 0.0193       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.619       |\n",
            "|    explained_variance   | 0.139        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 26.5         |\n",
            "|    n_updates            | 24           |\n",
            "|    policy_gradient_loss | -0.00441     |\n",
            "|    value_loss           | 65           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 49.3         |\n",
            "|    ep_rew_mean          | 49.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 159          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 102          |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041243876 |\n",
            "|    clip_fraction        | 0.0256       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.629       |\n",
            "|    explained_variance   | 0.461        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 30.1         |\n",
            "|    n_updates            | 28           |\n",
            "|    policy_gradient_loss | -0.00558     |\n",
            "|    value_loss           | 72.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 56.5        |\n",
            "|    ep_rew_mean          | 56.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 113         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003565079 |\n",
            "|    clip_fraction        | 0.0265      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.611      |\n",
            "|    explained_variance   | 0.197       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 30.3        |\n",
            "|    n_updates            | 32          |\n",
            "|    policy_gradient_loss | -0.00566    |\n",
            "|    value_loss           | 84.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 62           |\n",
            "|    ep_rew_mean          | 62           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 165          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 123          |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034692287 |\n",
            "|    clip_fraction        | 0.00989      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.614       |\n",
            "|    explained_variance   | 0.213        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 28.1         |\n",
            "|    n_updates            | 36           |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    value_loss           | 74.3         |\n",
            "------------------------------------------\n",
            "‚úÖ Training finished.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;117m[RemoteRL] | trainer |   5373 MB left | https://remoterl.com/user/dashboard | elapsed 0:00:00 | ~ calculating...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ Cell B ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import remoterl, os\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "# ------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Retrieve API key injected by the helper or set in the shell\n",
        "# ------------------------------------------------------------------\n",
        "key = os.getenv(\"REMOTERL_API_KEY\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Connect to the RemoteRL backend in trainer mode\n",
        "# ------------------------------------------------------------------\n",
        "if not remoterl.init(api_key=key, role=\"trainer\"):\n",
        "    raise RuntimeError(\"Failed to connect to RemoteRL.\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Build a vectorised CartPole environment (32 parallel instances)\n",
        "# ------------------------------------------------------------------\n",
        "ENV_ID = \"CartPole-v1\"\n",
        "env     = make_vec_env(ENV_ID, n_envs=32)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ Instantiate PPO with a modest network architecture\n",
        "# ------------------------------------------------------------------\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    policy_kwargs=dict(net_arch=dict(pi=[128, 64], vf=[128, 64])),\n",
        "    n_steps=64, n_epochs=4, batch_size=64, verbose=1, device=\"auto\",\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ Train for roughly 20k environment steps\n",
        "# ------------------------------------------------------------------\n",
        "model.learn(total_timesteps=20_000)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ Graceful shutdown of envs and confirmation message\n",
        "# ------------------------------------------------------------------\n",
        "env.close()\n",
        "print(\"‚úÖ Training finished.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rrl-tester",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
