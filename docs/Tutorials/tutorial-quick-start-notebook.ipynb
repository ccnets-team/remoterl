{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc649e68",
      "metadata": {},
      "source": [
        "# RemoteRL Quick-Start: Train a CartPole Agent Remotely with Stable-Baselines3\n",
        "\n",
        "## Introduction\n",
        "\n",
        "> In this notebook, we will train a classic **CartPole-v1** reinforcement learning agent remotely using Stable-Baselines3. This means the environment (CartPole) will run on a separate process or remote machine (the *simulator*), while the training algorithm runs on your main processor or local machine (the *trainer*).\n",
        "> **RemoteRL** makes this easy: just call `remoterl.init()` in your code to link the trainer and simulator through the RemoteRL cloud service. Once connected, every Gym environment command from the trainer (e.g. `reset`, `step`) executes on the remote simulator seamlessly, with no manual networking setup.\n",
        "\n",
        "1. **Setup: Install Dependencies**\n",
        "2. **Get an API Key**\n",
        "3. **Launch a Remote Simulator**\n",
        "4. **Start the Trainer and Train the Agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defbb50c",
      "metadata": {},
      "source": [
        "## 1. Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2b3fb0f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['init', 'shutdown']\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet remoterl stable-baselines3\n",
        "import remoterl, stable_baselines3 as sb3\n",
        "print(remoterl.__all__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e706b8e",
      "metadata": {},
      "source": [
        "*The command above installs the RemoteRL python library and Stable-Baselines3.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febc415f",
      "metadata": {},
      "source": [
        "## 2. Get an API Key\n",
        "\n",
        "RemoteRL requires an API key to connect to its cloud service. If you don't have one, you can create a free account on the RemoteRL website to obtain your API key.\n",
        "\n",
        "*Each free account includes **1‚ÄØGB** of data per month (approximately 1 million CartPole steps).*\n",
        "\n",
        "After signing up, copy your API key. *The next code cell will open your RemoteRL dashboard in a browser (if it isn‚Äôt already open) and prompt you to enter your API key into this notebook.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6f62c3b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, webbrowser\n",
        "DASHBOARD_URL = \"https://remoterl.com/user/dashboard\"\n",
        "\n",
        "def register_api_key(open_browser=True):\n",
        "    if open_browser:\n",
        "        webbrowser.open_new_tab(DASHBOARD_URL)\n",
        "        key = input(\"Paste your REMOTERL API key: \").strip()\n",
        "        os.environ[\"REMOTERL_API_KEY\"] = key\n",
        "    else:\n",
        "        key = os.getenv(\"REMOTERL_API_KEY\")\n",
        "    if not key:\n",
        "        print(f\"Please visit {DASHBOARD_URL} to get your API key.\")\n",
        "        raise RuntimeError(\"API key required.\")\n",
        "\n",
        "    print(\"‚úÖ RemoteRL registered. Happy training!\")\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38d6b301",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RemoteRL registered. Happy training!\n"
          ]
        }
      ],
      "source": [
        "# If the browser window doesn‚Äôt open automatically, visit the dashboard:\n",
        "DASHBOARD_URL = \"https://remoterl.com/user/dashboard\"\n",
        "\n",
        "key = register_api_key(open_browser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Launch a Remote Simulator\n",
        "\n",
        "Now we'll launch a **simulator** process to host the CartPole environment remotely. In this demo, the simulator will run as a separate background process on this machine. (In a real-world scenario, you could run the simulator on another machine or in the cloud just as easily.)\n",
        "\n",
        "Once the simulator starts, it will connect to the RemoteRL service and wait for the trainer to join. You should see log output in the cell indicating that the simulator is connected and ready to handle environment steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21f3d89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ  Simulator subprocess started (pid=38004)\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Simulator started, waiting for connection to Trainers...\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sim] \u001b[38;5;71m[RemoteRL] Connected | trainer=cbb65cd2\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Session started | trainer=cbb65cd2 | num_env_runners=2\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=1 make:{'0': 'CartPole-v1'} | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=1 make:{'0': 'CartPole-v1'} | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=8 make:{'7': 'CartPole-v1'} | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=8 make:{'7': 'CartPole-v1'} | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=64 step | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=64 step | runner=1\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=512 reset | runner=0\u001b[0m\n",
            "[sim] \u001b[38;5;71m[RemoteRL] Remote Environment | seq=512 reset | runner=1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ Cell A ¬∑ starts an isolated simulator (works on Windows, Linux, macOS) ‚îÄ‚îÄ\n",
        "import os, sys, textwrap, subprocess, time\n",
        "import threading\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Grab API key from the environment (set earlier by register_api_key)\n",
        "# ------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"REMOTERL_API_KEY\", key)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Build one‚Äëliner Python code that will run inside the child process\n",
        "#     (`remoterl.init(..., role=\"simulator\")` is intentionally blocking)\n",
        "# ------------------------------------------------------------------\n",
        "sim_code = textwrap.dedent(f\"\"\"\n",
        "    import remoterl\n",
        "    \n",
        "    remoterl.init(api_key='{API_KEY}', role='simulator')   # blocks here(Simulator init designed to be blocking)\n",
        "\"\"\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Spawn the simulator subprocess (same Python executable, unbuffered)\n",
        "#     - stdout/stderr are piped so we can echo logs back in real time\n",
        "# ------------------------------------------------------------------\n",
        "sim_proc = subprocess.Popen(\n",
        "    [sys.executable, \"-u\", \"-c\", sim_code],\n",
        "    stdout=subprocess.PIPE,            # stream simulator logs to parent\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Background thread: continuously forward simulator output\n",
        "# ------------------------------------------------------------------\n",
        "def stream_logs(proc):\n",
        "    for line in iter(proc.stdout.readline, ''):   # keep reading until EOF\n",
        "        if line:\n",
        "            print(f\"[sim] {line.rstrip()}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£  Confirmation + optional head‚Äëstart delay\n",
        "# ------------------------------------------------------------------\n",
        "log_thread = threading.Thread(target=stream_logs, args=(sim_proc,), daemon=True)\n",
        "log_thread.start()\n",
        "\n",
        "print(f\"üöÄ  Simulator subprocess started (pid={sim_proc.pid})\")\n",
        "\n",
        "time.sleep(10)                          # give it a head-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Start the Online Trainer\n",
        "\n",
        "Finally, we start the **trainer** process to train our agent using the remote environment. We‚Äôll use the **PPO** algorithm (Proximal Policy Optimization) from Stable-Baselines3 to train the CartPole agent. The code below will initialize the trainer session (using your API key and connecting to RemoteRL), create a parallelized CartPole environment (32 simultaneous instances running on the remote simulator to speed up training), and then begin training the agent.\n",
        "\n",
        "When you run this cell, you will see training progress logs (from Stable-Baselines3) appear in the output. After about 20,000 timesteps, the training will complete and you should see a \"‚úÖ Training finished.\" message, indicating that our remote training demo is successfully finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;71m[RemoteRL] Session opened | trainer=cbb65cd2 | num_env_runners=2 | num_workers=1\u001b[0m\n",
            "\u001b[94m[RemoteRL] Remote Gym enabled with 1 workers and 2 runners.\u001b[0m\n",
            "\u001b[94m[RemoteRL] Remote Stable-Baselines3 applied.\u001b[0m\n",
            "Using cpu device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 20.2     |\n",
            "|    ep_rew_mean     | 20.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 139      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 24.1        |\n",
            "|    ep_rew_mean          | 24.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 133         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 30          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004036289 |\n",
            "|    clip_fraction        | 0.0115      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.691      |\n",
            "|    explained_variance   | -0.00612    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18.4        |\n",
            "|    n_updates            | 4           |\n",
            "|    policy_gradient_loss | -0.00374    |\n",
            "|    value_loss           | 68.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 29          |\n",
            "|    ep_rew_mean          | 29          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 140         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 43          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002507267 |\n",
            "|    clip_fraction        | 0.00171     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.684      |\n",
            "|    explained_variance   | 0.0222      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.78        |\n",
            "|    n_updates            | 8           |\n",
            "|    policy_gradient_loss | -0.00199    |\n",
            "|    value_loss           | 37.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 34.8        |\n",
            "|    ep_rew_mean          | 34.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 144         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004116657 |\n",
            "|    clip_fraction        | 0.0219      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.663      |\n",
            "|    explained_variance   | 0.0513      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14.7        |\n",
            "|    n_updates            | 12          |\n",
            "|    policy_gradient_loss | -0.00805    |\n",
            "|    value_loss           | 37.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 38.9        |\n",
            "|    ep_rew_mean          | 38.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 149         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002572057 |\n",
            "|    clip_fraction        | 0.0211      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.652      |\n",
            "|    explained_variance   | 0.0014      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 26.8        |\n",
            "|    n_updates            | 16          |\n",
            "|    policy_gradient_loss | -0.00643    |\n",
            "|    value_loss           | 51.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 44.3        |\n",
            "|    ep_rew_mean          | 44.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 153         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 80          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002521756 |\n",
            "|    clip_fraction        | 0.00513     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.639      |\n",
            "|    explained_variance   | 0.0728      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 22.1        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.002      |\n",
            "|    value_loss           | 65.1        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 50.7         |\n",
            "|    ep_rew_mean          | 50.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 156          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 91           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035563617 |\n",
            "|    clip_fraction        | 0.0105       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.635       |\n",
            "|    explained_variance   | 0.146        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 37.2         |\n",
            "|    n_updates            | 24           |\n",
            "|    policy_gradient_loss | -0.00228     |\n",
            "|    value_loss           | 73.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 59.3        |\n",
            "|    ep_rew_mean          | 59.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 160         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 101         |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002921715 |\n",
            "|    clip_fraction        | 0.00916     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.619      |\n",
            "|    explained_variance   | 0.175       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 42.2        |\n",
            "|    n_updates            | 28          |\n",
            "|    policy_gradient_loss | -0.0031     |\n",
            "|    value_loss           | 76.6        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 69.5          |\n",
            "|    ep_rew_mean          | 69.5          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 163           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 112           |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00095884176 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.622        |\n",
            "|    explained_variance   | 0.634         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 22.8          |\n",
            "|    n_updates            | 32            |\n",
            "|    policy_gradient_loss | -0.000819     |\n",
            "|    value_loss           | 64            |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 81.8         |\n",
            "|    ep_rew_mean          | 81.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 166          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 122          |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014596403 |\n",
            "|    clip_fraction        | 0.011        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.614       |\n",
            "|    explained_variance   | 0.246        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 52.4         |\n",
            "|    n_updates            | 36           |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    value_loss           | 84           |\n",
            "------------------------------------------\n",
            "‚úÖ Training finished.\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ Cell B ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import remoterl, os\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "# ------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Retrieve API key injected by the helper or set in the shell\n",
        "# ------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"REMOTERL_API_KEY\", key)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Connect to the RemoteRL backend in trainer mode\n",
        "# ------------------------------------------------------------------\n",
        "if not remoterl.init(api_key=API_KEY, role=\"trainer\"):\n",
        "    raise RuntimeError(\"Failed to connect to RemoteRL.\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Build a vectorised CartPole environment (32 parallel instances)\n",
        "# ------------------------------------------------------------------\n",
        "ENV_ID = \"CartPole-v1\"\n",
        "env     = make_vec_env(ENV_ID, n_envs=32)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ Instantiate PPO with a modest network architecture\n",
        "# ------------------------------------------------------------------\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    policy_kwargs=dict(net_arch=dict(pi=[128, 64], vf=[128, 64])),\n",
        "    n_steps=64, n_epochs=4, batch_size=64, verbose=1, device=\"auto\",\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ Train for roughly 20k environment steps\n",
        "# ------------------------------------------------------------------\n",
        "model.learn(total_timesteps=20_000)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ Graceful shutdown of envs and confirmation message\n",
        "# ------------------------------------------------------------------\n",
        "env.close()\n",
        "print(\"‚úÖ Training finished.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tester",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
