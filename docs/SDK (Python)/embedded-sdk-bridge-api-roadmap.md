# Embedded Simulator-SDK Bridge API Roadmap (from v1.2.0)

## Overview

**RemoteRL** is a cloud-based platform that decouples the agent’s training process from environment execution, enabling reinforcement-learning (RL) training to run remotely. In essence, it lets you **connect environments (simulators or robots) from anywhere** and stream live experience data to train models in real time. This document outlines the API roadmap for the next version of RemoteRL Simulator-SDK, which focuses on supporting **massive real-world environment training** using speread embedded devices. We introduce new API functions to make it easy to coordinate many remote simulators, schedule resources, and ensure reliable training across a distributed fleet of devices.


## Planned API Functions (next version)

To coordinate distributed trainers and simulators and manage resource allocation at scale, the next version of RemoteRL introduces several API calls in the Python SDK(on the trainer side). These calls let your code label devices, discover available simulators, and reserve a set of environments for a training session. Below is a summary of each function and its usage:

* **`remoterl.init(api_key, role, labels=set(), **extras) → trainer_id or simulator_id`** — Register this instance to RemoteRL server as either a **trainer** or **simulator**. This returns a unique `node_id` from the remoterl cloud server. You can optionally attach a set of `labels` (e.g. `{"gpu", "lab-2"}`) to describe the node’s capabilities or location. **When to call:** at process startup.

* **`remoterl.describe(api_key, role=None, ids=None, labels=None) → dict`** — Query for a **live inventory** of connected simulators or trainers with specific node IDs, or by label selectors. The returned dictionary lists active nodes and their metadata (IDs, roles, labels, etc.) under your API key. **When to use:** on-demand for discovery or monitoring – e.g. to check how many simulators are currently online or find their IDs before starting a training session. 

* **`remoterl.check(trainer_id, num_simulators=None, num_workers=None, num_env_runners=None, num_envs=None, labels=None) → bool`** — Perform a **dry-run capacity probe** to ask the server *“Could I reserve the requested resources right now?”*. This lets a trainer script verify that enough simulator resources are available *before* committing to a training session. You can specify how many simulators (`num_simulators`), how many environment runner processes (`num_env_runners`), and/or how many total environment instances (`num_envs`) you need, as well as any label requirements. The server will validate a reservation would work with those parameters and return `True` if it could be satisfied (all requested slots are free), or `False` if not. **When to use:** right before attempting a real reservation – for example, your training program can call `check()` and only proceed if it returns true, thereby avoiding partially starting a job that might later stall due to insufficient environments. 

* **`remoterl.reserve(trainer_id, num_simulators=None, num_workers=None, num_env_runners=None, num_envs=None, labels=None, ttl=30) → token`** — If `check()` indicates resources are available, you call `reserve()` to **lock in those resources** for your session. This actually allocates the specified number of simulator workers, environment runners, and concrete environment instances to your trainer. It returns a session **token**, which the trainer and simulators will use to confirm that they belong to the same session. The reservation is held for a limited time (TTL, default 30 seconds) before it expires – you should start creating your environments within that time window.**When to use:** By reserving, you ensure that the required simulators and environment slots are **exclusively assigned for that trainer caller** for the next stage. If the trainer fails to initiate the session (e.g. crashes or takes too long), the reservation will automatically expire after the TTL so that other trainers can use the resources.

* **`remoterl.release(token)`** — Release a previously made reservation, freeing up the simulators and other resources so they can be used by another job or trainer. You provide the same session `token` that was returned by `reserve()`. **When to use:** **just after training session starts**. 

## Typical Workflow Example

Using the above API, a typical training workflow with remote environments would look like this:

1. **Start simulators:** Launch the simulator processes (e.g. on your edge devices or other machines). Each simulator calls `remoterl.init(api_key, role="simulator", labels=...)` as it starts up. After this, they appear as connected and idle via the API key (and will show up in `describe()` queries with the same API key).
2. **Initialize trainer:** Launch the training script/process. The trainer calls `remoterl.init(api_key, role="trainer")` during its startup, obtaining a `trainer_id` that represents the trainer’s connection. At this point, the trainer is connected to the remoterl server but not yet tied to any simulator(and environments).
3. **Probe capacity:** The trainer calls `remoterl.check(trainer_id, num_simulators=X, num_workers=Y, num_envs=Z, labels=...)` to ensure that the desired number of simulators/environments (e.g. X simulators hosting Y environments) are currently available. If this returns `True`, the trainer knows it *can* get those resources. If `False`, the trainer might decide to wait, reduce the request, or abort (for instance, if not enough simulators are online, you wouldn’t want to start and get stuck waiting).
4. **Reserve resources:** Assuming the check passed, the trainer now calls `remoterl.reserve(trainer_id, num_simulators=X, num_workers=Y, num_envs=Z, labels=..., ttl=30)`. This locks in X simulators (and the ability to create Z environments on them) for this trainer and returns a session `token`. The simulators are internally notified by the server that they’ve been “assigned” to this trainer session.
5. **Create environments and train:** With a valid reservation token, the trainer can safely create environment instances. In code, this might simply mean calling your RL library’s normal environment constructor (e.g. `gym.make(...)`), which under the hood will communicate with the simulators (via the RemoteRL SDK) to **spawn actual environment runners** on those remote Simulator nodes. Each simulator, upon receiving a create-environment request, will instantiate the environment (e.g. connect to a game instance or robot interface) and begin exchanging data. Now the typical RL loop runs: the trainer sends actions and receives observations/rewards from each remote environment step through the remoterl relay.


## Benefits for Large-Scale & Real-World Training

The real-time bridge API and architecture bring several key benefits for scaling RL training to many **real-world environments**:

* **Massive scale-out with central coordination:** RemoteRL makes it straightforward to connect dozens or even hundreds of simulators (or physical devices) to a single trainer, feeding experience data in parallel. The new `check/reserve` mechanism lets you reliably acquire all the resources you need for a training run, or wait if they’re not available, which is crucial when managing large experiments. The system handles the fan-in of data and fan-out of actions under the hood – you can scale from a single environment to hundreds without changing your training code.

* **Minimal intrusion and easy integration:** Integrating an existing simulator or robot with RemoteRL doesn’t require a rewrite of its logic. The game or robot developer can implement a subclass with a few callback methods (to generate observations, calculate rewards, and apply actions), and links the RemoteRL library(SDK-Sim) into their application. All the networking, threading, and synchronization is handled by RemoteRL, so the focus is only on exposing the environment’s core logic.

Overall, the upcoming **embedded Simulator-SDK bridge API** will enhance RemoteRL’s ability to serve as a **“training-from-anywhere” backbone** for reinforcement learning. By allowing dynamic discovery and reservation of distributed environments, it supports flexible scaling from one to many devices. By using a globally optimized relay and smart scheduling, it maintains real-time performance even as you scale up. And with the built-in safety mechanisms (like the control router and resource TTLs), it ensures that even massive, geographically dispersed experiments run **smoothly and safely** – unlocking the value of *massive real-world remote training* for robotics, IoT, and beyond.
