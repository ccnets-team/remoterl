# RemoteRL Sim-SDK — Immediate Benefits for Game Comanpies

RemoteRL Sim-SDK is a plug-and-play cloud service that drops reinforcement learning into any Unity, Unreal, or custom C++ game. **By turning every game user into a training worker, it taps your live player base for compute cycles and fresh telemetry, so agents keep getting smarter long after launch.** With a single API key and a lightweight SDK, you get zero-setup, auto-scaling RL: the cloud spins up thousands of parallel trainers, runs your game or simulation, and delivers production-ready agents trained in real-world environments.

## Key Benefits and Use Cases

RemoteRL lets **game studios, AI researchers, sim publishers, and engineering teams** use reinforcement learning with almost no setup. For project managers, it means quicker delivery and smarter AI features. **Key benefits:**

* **Simulators run online—always under your control.** Embed the SDK in your game or sim build, launch it in your own environment (PC, console dev-kit, mobile device, or CI farm), and each instance registers itself with RemoteRL as a “simulator.” All code and assets stay local; only lightweight observations and actions stream to the cloud, so your IP never leaves your machines.
* **Training runs in the cloud.** Launch a trainer session (SB3, RLlib) with the RemoteRL Python package (only the `trainer` module will be used). Our WebSocket server mediates every step–action exchange, so from your side it feels like a standard Gym loop while compute and scaling stay in the cloud.
* **Distributed rollout is built-in.** You can register many simulator instances (on one PC, on several PCs, or even via the provided Docker image) and the trainer pulls experience from all of them in parallel to accelerate learning.
* **Seamless Integration with your game Environment:** RemoteRL uses a standard Gymnasium (OpenAI Gym) style interface for environments, making it easy to connect your own game or simulator. You integrate a lightweight C++ SDK into your game client or simulator, and RemoteRL acts as a **gateway transferring environment states and actions** in real time between your environment and the cloud trainer. This means you can plug in existing environments (from simple 2D games to complex 3D simulations) with minimal code changes and immediately start remote training.
**Cross-Platform & Engine Support** — RemoteRL ships as a single, portable C library, so the **same SDK you build for Windows or Linux can be dropped straight into iOS and Android apps—and, with standard dev-kit toolchains, into flagship consoles like PlayStation 5, Nintendo Switch, and Xbox Series X|S**. Whether your project runs in Unity, Unreal, Godot, or a custom C++ engine, you link one binary, call a stable C API, and every desktop, mobile, and console build talks to the cloud trainers in exactly the same way—delivering production-grade RL on day one and future-proofing you for new hardware releases.

## From Training to Inference: End-to-End Solution

RemoteRL’s primary focus is to make **training** of reinforcement learning agents as easy and efficient as possible for the user. By handling the heavy lifting of distributed training, algorithm optimization, and cloud scaling, it frees your team to concentrate on defining the problem (designing the environment and reward signals) rather than reinventing RL infrastructure. This can drastically shorten the development cycle for AI features – what might have taken months to set up can often be achieved in days with RemoteRL.

However, the platform is not just about training; it is an **end-to-end solution** that also covers what comes after training: **inference and deployment**. Once your agents are trained to the desired performance level, RemoteRL offers flexible ways to put them into action:

* **On-Device Inference:** Train the model, export it, and embed it at build time with `rrl_load_policy`, so your engineers compile the policy right alongside the game’s source code. The integration layer is left **open and extensible**, giving teams room to add custom data transforms, memory allocators, or telemetry hooks. Because the model ships inside the binary, all decisions run locally—delivering instant responses even offline, perfect for twitch-speed games or robots—while the lightweight C++ SDK keeps performance snappy.
* **Cloud-side inference:** Train once, upload the model to RemoteRL’s cloud, and call it whenever you need. As an alternative to on-device inference, your game streams the current state to the cloud over a WebSocket via the SDK, and the service streams back the next action. Because the model is hosted centrally, you can upgrade the AI without shipping new client builds—ideal for live games or mobile/IoT devices. Clients stay lightweight; you just incur a bit of network latency, so use this mode where a stable connection is available (turn-based play, assist features, etc.).
* **Continuous Improvement:** RemoteRL supports both training and serving through the Sim-SDK embedded in your distributed games, so you can run a **continuous learning loop**. Periodically retrain on fresh gameplay data, then push the updated model to production—all within the RemoteRL ecosystem. The result is adaptive AI that grows smarter over time, whether for evolving game opponents or for long-running simulations and services.

RemoteRL Sim-SDK removes the heavy lifting of RL engineering and cloud ops, so teams can focus on building cool AI features. With one toolkit you train in the cloud, deploy anywhere, and scale securely—whether that’s smarter game NPCs, autonomous drones, or interactive sims. Unified training + inference, broad integration, and enterprise-grade reliability make it a fast, future-proof path to bringing RL agents to life.

**RemoteRL is transforming the RL workflow** – making it easier to go from an idea to a trained, deployable AI agent that works across **“from Unity to Unreal Engine and other platforms” with smooth performance**. For anyone seeking to leverage the potential of reinforcement learning in gaming or simulation products, RemoteRL offers a compelling, future-proof solution.
